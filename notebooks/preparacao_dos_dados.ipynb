{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação dos dados para modelagem\n",
    "\n",
    "Na seção anterior realizamos uma serie de analise descritiva dos dados com o intuito de entender a estrutura do dados e identificar as variáveis que podem ser usadas para prever se um cliente vai abandonar o banco ou continuar com ele.\n",
    "\n",
    "Neste etapa, iremos realizar algumas transformações nos dados para que eles sejam adequados ao modelo preditivo.\n",
    "\n",
    "Um ponto que deve ser ressaltado, temos dois banco de dados **Abandono_clientes.csv** e **Abandono_teste.csv**, o primeiro foi o banco de dados que usamos na etapa anterior e será este que iremos usar para treinar e testar o modelo. O segundo banco de dados, será utilizado apenas no final para avaliar o desempenho do modelo.\n",
    "\n",
    "Para construção do nosso modelo utilizaremos o método de [Previsão Conforme](https://medium.com/data-hackers/uma-introdu%C3%A7%C3%A3o-pr%C3%A1tica-%C3%A0-previs%C3%A3o-conforme-de4c7479e021), o objetivo de usar este método em conjunto com outros métodos preditivos é obter um nível de confiança estatística que nos permita avaliar o desempenho do nosso modelo. Desta forma, vamos poder quantificar a incerteza e avaliar o desempenho do nosso modelo.\n",
    "\n",
    "\n",
    "Nesta fase iremos utilizar uma das principais bibliotecas no cenário de ciência de dados, o [scikit-lerarn](https://scikit-learn.org/) juntamente com os seus algoritmos de machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "\n",
    "# Manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# Visualização\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import HTML \n",
    "import plotly.graph_objects as go \n",
    "\n",
    "# Algumas configurações\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Configuração para o notebook e plotagem de imagens\n",
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    plt.style.use('bmh')\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    display(HTML('<style>.container { width:100% !important; }</style>'))\n",
    "    sns.set()\n",
    "\n",
    "jupyter_settings()\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# Processamento dos dados\n",
    "from sklearn.model_selection import train_test_split # Divisão dos dados em treino e teste\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Codifica variáveis categóricas em valores numéricos.\n",
    "\n",
    "# LabelEncoder, OneHotEncoder ->Codifica variáveis categóricas em valores numéricos.\n",
    "# O primeiro identifica em número inteiro único e não altera a dimensionalidade, mas pode introduzir ordem artificial\n",
    "# O segunda cada categoria é uma coluna binária e aumenta a dimensionalidade, mas não há ordem artificial\n",
    "# Normaliza os dados numéricos para facilitar o treinamento dos modelos.\n",
    " \n",
    "from sklearn.compose import ColumnTransformer # Permite aplicar transformações diferentes para diferentes colunas\n",
    "from sklearn.pipeline import Pipeline # Permite combinar múltiplas etapas do processo de machine learning\n",
    "\n",
    "\n",
    "# Modelos de Machine Learning:\n",
    "from sklearn.tree import DecisionTreeClassifier # Importa o classificador baseado em árvores de decisão.\n",
    "from sklearn.ensemble import RandomForestClassifier # Importa o classificador baseado em florestas aleatórias.\n",
    "from sklearn.naive_bayes import GaussianNB #  Importa o classificador Naive Bayes Gaussiano.\n",
    "from sklearn.neighbors import KNeighborsClassifier # Importa o classificador K-Nearest Neighbors (KNN).\n",
    "from sklearn.svm import SVC # Importa o classificador de Máquinas de Vetores de Suporte (SVM).\n",
    "from sklearn.neural_network import MLPClassifier # Importa o classificador baseado em redes neurais artificiais (Perceptron Multicamadas).\n",
    "from sklearn.ensemble import AdaBoostClassifier # Importa o classificador baseado em AdaBoost.\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Importa o classificador baseado em Gradient Boosting.\n",
    "from sklearn.ensemble import ExtraTreesClassifier # Importa o classificador baseado em Árvores Extras (Extra Trees).\n",
    "from xgboost import XGBClassifier #  Importa o classificador baseado no XGBoost, um algoritmo de Gradient Boosting otimizado.\n",
    "from catboost import CatBoostClassifier # Importa o classificador baseado no CatBoost, especializado em dados categóricos.\n",
    "from sklearn.linear_model import LogisticRegression # Importa o modelo de Regressão Logística.\n",
    "\n",
    "# Métricas de desempenho\n",
    "# Calcula a taxa de recuperação (recall).\n",
    "# Calcula a curva ROC para análise de desempenho.\n",
    "# Gera a matriz de confusão.\n",
    "# Calcula a precisão do modelo\n",
    "# Calcula a métrica F1, que é a média harmônica entre precisão e recall.\n",
    "# Calcula a acurácia do modelo.\n",
    "# Gera um relatório detalhado com precisão, recall e F1-score para cada classe.\n",
    "from sklearn.metrics import recall_score, roc_curve, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que iremos remover as seguintes variáveis`RowNumber`', `CustomerId`, `Surname`, pois elas não tem papel nenhum na predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('../data/Abandono_clientes.csv', sep=',')\n",
    "# Vamos fazer uma copia do dataset original\n",
    "\n",
    "train_transform = df_raw.copy()\n",
    "train_transform.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True) \n",
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos separar os dados na variável algo $y=$ **Exited** e as variáveis preditoras $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_transform['Exited']\n",
    "\n",
    "X = train_transform.drop('Exited', axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dividir os dados da seguinte forma:\n",
    "\n",
    "- 7000 amostras de dados (\"`X_train`\", \"`y_train`\") para treinar o modelo;\n",
    "- 1000 amostras de dados (\"`X_test`\", \"`y_test`\") para avaliar o desempenho do modelo em dados \"não vistos\" no treinamento;\n",
    "- 1000 amostras de dados (\"`X_calib`\"`y_calib`\") para calibração em dados \"não vistos\" no treinamento;\n",
    "- 1000 amostras restantes (\"`X_new`\"`y_new`\") para etapa de previsão conforme e para sua avaliação.\n",
    "\n",
    "Inicialmente iremos fazer esta distribuição, a depender dos resultados, podemos voltar e alterar estes valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are characters but should be integers for sklearn\n",
    "\n",
    "# Split dos dados em treino e teste\n",
    "X_train, X_rest1, y_train, y_rest1 = train_test_split(X, y, train_size=7000, random_state=2)\n",
    "\n",
    "# Dos dados que sobraram, separando em dados de teste\n",
    "X_test, X_rest2, y_test, y_rest2 = train_test_split(X_rest1, y_rest1, train_size=1000, random_state=42)\n",
    "\n",
    "# Dos dados que sobraram, separando em calibração e new\n",
    "X_calib, X_new, y_calib, y_new = train_test_split(X_rest2, y_rest2, train_size=1000, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição de uma baseline\n",
    "\n",
    "O intuito de criarmos uma baseline é para termos um ponto de partida ao avaliar o desempenho do nosso modelo. Iremos utilizar uma métrica que faz uso de Classificador de taxa aleatória (estimativa ponderada), para ver mais acesse o [artigo](https://towardsdatascience.com/calculating-a-baseline-accuracy-for-a-classification-model-a4b342ceb88f). Desta forma teremos a ZeroR baseline que a taxa mais frequente e a Random Rate (Baseline Ponderada) que é construída a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporção da Classe Minoritária (Exited = 1): 0.2074\n",
      "Proporção da Classe Majoritária (Exited = 0): 0.7926\n",
      "\n",
      "Odds de Adivinhar Corretamente a Classe Minoritária: 0.20742857142857143^2 = 0.0430\n",
      "Odds de Adivinhar Corretamente a Classe Majoritária: 0.7925714285714286^2 = 0.6282\n",
      "\n",
      "Baseline Ponderado: 0.6712 ou 67.12%\n"
     ]
    }
   ],
   "source": [
    "# Suponha que `y_train` contém os rótulos da variável alvo\n",
    "# Cálculo das proporções de cada classe\n",
    "proporcoes = np.bincount(y_train) / len(y_train)\n",
    "p_minoritaria = proporcoes[1]  # Proporção da classe Exited = 1\n",
    "p_majoritaria = proporcoes[0]  # Proporção da classe Exited = 0\n",
    "\n",
    "# Cálculo das odds. Probabilidade de adivinhar corretamente com base nas proporções.\n",
    "odds_minoria = p_minoritaria**2\n",
    "odds_maioria = p_majoritaria**2\n",
    "\n",
    "# Soma as odds ponderadas para cada classe para obter a precisão esperada de um classificador que adivinha com base nas proporções.\n",
    "baseline = odds_minoria + odds_maioria\n",
    "\n",
    "# Exibição dos resultados\n",
    "print(f\"Proporção da Classe Minoritária (Exited = 1): {p_minoritaria:.4f}\")\n",
    "print(f\"Proporção da Classe Majoritária (Exited = 0): {p_majoritaria:.4f}\")\n",
    "print()\n",
    "print(f\"Odds de Adivinhar Corretamente a Classe Minoritária: {p_minoritaria}^2 = {odds_minoria:.4f}\")\n",
    "print(f\"Odds de Adivinhar Corretamente a Classe Majoritária: {p_majoritaria}^2 = {odds_maioria:.4f}\")\n",
    "print()\n",
    "print(f\"Baseline Ponderado: {baseline:.4f} ou {baseline * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que nossa Baseline Ponderada é de $67,12\\%$ e a ZeroR Baseline é de $79,26\\%$ estes valores serão os valores de avaliação inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação dos dados\n",
    "\n",
    "Vamos fazer a separação das variáveis numéricas e categóricas. Decidi adicionar `HasCrCard`, `IsActiveMember` como variáveis categóricas, mesmo sendo números. No futuro posso voltar aqui e rever esta decisão. \n",
    "\n",
    "As transformações que vamos fazer são:\n",
    "\n",
    "- **StandardScaler** para padronizar os valores, inicialmente não irei tratar os outliers, desejo ver como os modelos atual considerando esses valores, também não irei o min-max scaler para os valores que não possuem outliers.\n",
    "- **OneHotEncoder** para transformar as variáveis categóricas do tipo `Gender` e `Geography` e valores binários. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis numéricas categóricas\n",
    "categorical_numeric_features = ['HasCrCard', 'IsActiveMember', 'NumOfProducts']\n",
    "\n",
    "# Variáveis numéricas contínuas\n",
    "numeric_features = [col for col in X_train.select_dtypes(include=np.number).columns if col not in categorical_numeric_features]\n",
    "\n",
    "# Variáveis categóricas\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist() + categorical_numeric_features\n",
    "\n",
    "# Transformadores\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Pré-processador\n",
    "preprocesso = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Criação do pipeline para conversão\n",
    "preprocesso = ColumnTransformer(transformers= [('num', numeric_transformer, numeric_features),\n",
    "                                            ('cat', categorical_transformer, categorical_features)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem \n",
    "\n",
    "Vamos criar uma lista com os seguintes modelos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo uma lista de modelos\n",
    "list_models = [('LOGISTIC REGRESSION', LogisticRegression(solver='lbfgs')),\n",
    "               ('DECISION TREE', DecisionTreeClassifier()),\n",
    "               ('KNN', KNeighborsClassifier()),\n",
    "               ('SVC', SVC()),\n",
    "               ('RANDOM FOREST', RandomForestClassifier(n_estimators=1000)),\n",
    "               ('XGB', XGBClassifier(n_estimators=300, eval_metric='logloss', seed=42)),\n",
    "               ('ABC', AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=42)),\n",
    "               ('ExT', ExtraTreesClassifier()),\n",
    "               ('GBC', GradientBoostingClassifier()),\n",
    "               ('CatBC', CatBoostClassifier(verbose=0))\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "------------------------------------------------------\n",
      "Treino \t 0.842\n",
      "Validação 0.846 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91       793\n",
      "           1       0.73      0.40      0.52       207\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.80      0.68      0.71      1000\n",
      "weighted avg       0.83      0.85      0.83      1000\n",
      "\n",
      "====================================================== \n",
      "\n",
      "DECISION TREE\n",
      "------------------------------------------------------\n",
      "Treino \t 1.0\n",
      "Validação 0.816 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88       793\n",
      "           1       0.55      0.57      0.56       207\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.72      0.73      0.72      1000\n",
      "weighted avg       0.82      0.82      0.82      1000\n",
      "\n",
      "====================================================== \n",
      "\n",
      "KNN\n",
      "------------------------------------------------------\n",
      "Treino \t 0.8705714285714286\n",
      "Validação 0.842 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.91       793\n",
      "           1       0.73      0.37      0.49       207\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.79      0.67      0.70      1000\n",
      "weighted avg       0.83      0.84      0.82      1000\n",
      "\n",
      "====================================================== \n",
      "\n",
      "SVC\n",
      "------------------------------------------------------\n",
      "Treino \t 0.8667142857142857\n",
      "Validação 0.864 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92       793\n",
      "           1       0.83      0.43      0.57       207\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.85      0.70      0.74      1000\n",
      "weighted avg       0.86      0.86      0.85      1000\n",
      "\n",
      "====================================================== \n",
      "\n",
      "RANDOM FOREST\n",
      "------------------------------------------------------\n",
      "Treino \t 1.0\n",
      "Validação 0.875 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92       793\n",
      "           1       0.82      0.51      0.63       207\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.85      0.74      0.78      1000\n",
      "weighted avg       0.87      0.88      0.86      1000\n",
      "\n",
      "====================================================== \n",
      "\n",
      "XGB\n",
      "------------------------------------------------------\n",
      "Treino \t 1.0\n",
      "Validação 0.866 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       793\n",
      "           1       0.74      0.54      0.63       207\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.81      0.75      0.77      1000\n",
      "weighted avg       0.86      0.87      0.86      1000\n",
      "\n",
      "====================================================== \n",
      "\n",
      "ABC\n",
      "------------------------------------------------------\n",
      "Treino \t 0.8585714285714285\n",
      "Validação 0.865 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       793\n",
      "           1       0.78      0.49      0.60       207\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.83      0.73      0.76      1000\n",
      "weighted avg       0.86      0.86      0.85      1000\n",
      "\n",
      "====================================================== \n",
      "\n",
      "ExT\n",
      "------------------------------------------------------\n",
      "Treino \t 1.0\n",
      "Validação 0.86 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.92       793\n",
      "           1       0.77      0.46      0.58       207\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.82      0.71      0.75      1000\n",
      "weighted avg       0.85      0.86      0.85      1000\n",
      "\n",
      "====================================================== \n",
      "\n",
      "GBC\n",
      "------------------------------------------------------\n",
      "Treino \t 0.8752857142857143\n",
      "Validação 0.877 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       793\n",
      "           1       0.81      0.53      0.64       207\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.85      0.75      0.78      1000\n",
      "weighted avg       0.87      0.88      0.87      1000\n",
      "\n",
      "====================================================== \n",
      "\n",
      "CatBC\n",
      "------------------------------------------------------\n",
      "Treino \t 0.9101428571428571\n",
      "Validação 0.875 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.92       793\n",
      "           1       0.80      0.53      0.64       207\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.84      0.75      0.78      1000\n",
      "weighted avg       0.87      0.88      0.86      1000\n",
      "\n",
      "====================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in list_models:\n",
    "    clf = Pipeline(steps=[('preprocesso', preprocesso),\n",
    "                          ('classificação', model)])\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    test_score = clf.score(X_test, y_test )\n",
    "\n",
    "    print(name)\n",
    "    print('-'*54)\n",
    "    print('Treino', '\\t', train_score)\n",
    "    print('Validação', test_score, '\\n')\n",
    "    print(classification_report(y_test, clf.predict(X_test)))\n",
    "    print('='*54, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.806\n",
      "          Não Saiu  Saiu\n",
      "Não Saiu       696    97\n",
      "Saiu            97   110\n"
     ]
    }
   ],
   "source": [
    "# Modelo \n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "# Check accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", (y_pred == y_test).mean())\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(pd.DataFrame(cm,  index=['Não Saiu', 'Saiu'],      # Rótulos das classes verdadeiras\n",
    "    columns=['Não Saiu', 'Saiu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Get the \"probabilities\" from the model\n",
    "predictions = model.predict_proba(X_calib)\n",
    "\n",
    "# Get for each instance the highest probability\n",
    "high_prob_predictions = np.amax(predictions, axis=1)\n",
    "\n",
    "# Select the predictions where probability over 99%\n",
    "high_p_Exited = np.where(high_prob_predictions >= 0.95)\n",
    "\n",
    "# Let's count how often we hit the right label\n",
    "its_a_match = (model.predict(X_calib) == y_calib)\n",
    "coverage = np.mean(its_a_match.values[high_p_Exited])\n",
    "print(round(coverage, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Geography', 'Gender']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhpedro/miniconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [23:12:54] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 7. Avaliar o pipeline\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m train_score \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m test_score \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcurácia no treino: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/pipeline.py:1191\u001b[0m, in \u001b[0;36mPipeline.score\u001b[0;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform the data, and apply `score` with the final estimator.\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03mCall `transform` of each transformer in the pipeline. The transformed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m    Result of calling `score` on the final estimator.\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# TODO(1.8): Remove the context manager and use check_is_fitted(self)\u001b[39;00m\n\u001b[0;32m-> 1191\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_raise_or_warn_if_not_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_routing_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/pipeline.py:60\u001b[0m, in \u001b[0;36m_raise_or_warn_if_not_fitted\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# we only get here if the above didn't raise\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError:\n\u001b[1;32m     62\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Pipeline instance is not fitted yet. Call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappropriate arguments before using other methods such as transform, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1756\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mrequires_fit \u001b[38;5;129;01mand\u001b[39;00m attributes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_or_any\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1665\u001b[0m, in \u001b[0;36m_is_fitted\u001b[0;34m(estimator, attributes, all_or_any)\u001b[0m\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_or_any([\u001b[38;5;28mhasattr\u001b[39m(estimator, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attributes])\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_is_fitted__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_is_fitted__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1667\u001b[0m fitted_attrs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1668\u001b[0m     v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1669\u001b[0m ]\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fitted_attrs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/pipeline.py:1321\u001b[0m, in \u001b[0;36mPipeline.__sklearn_is_fitted__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;66;03m# check if the last step of the pipeline is fitted\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;66;03m# we only check the last step since if the last step is fit, it\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# means the previous steps should also be fit. This is faster than\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;66;03m# checking if every step of the pipeline is fit.\u001b[39;00m\n\u001b[0;32m-> 1321\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1751\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m-> 1751\u001b[0m tags \u001b[38;5;241m=\u001b[39m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mrequires_fit \u001b[38;5;129;01mand\u001b[39;00m attributes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_tags.py:430\u001b[0m, in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39mmro()):\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[0;32m--> 430\u001b[0m         sklearn_tags_provider[klass] \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_tags__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    431\u001b[0m         class_order\u001b[38;5;241m.\u001b[39mappend(klass)\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_more_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/base.py:540\u001b[0m, in \u001b[0;36mClassifierMixin.__sklearn_tags__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_tags__\u001b[49m()\n\u001b[1;32m    541\u001b[0m     tags\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m     tags\u001b[38;5;241m.\u001b[39mclassifier_tags \u001b[38;5;241m=\u001b[39m ClassifierTags()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 2. Definir transformações\n",
    "numeric_transformer = StandardScaler()  # Padronizar variáveis numéricas\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')  # Codificar variáveis categóricas\n",
    "\n",
    "# 3. Criar o pré-processador\n",
    "preprocesso = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# 4. Definir o classificador com parâmetros recomendados\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# 5. Criar o pipeline completo\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocesso', preprocesso),\n",
    "    ('classificação', xgb_model)\n",
    "])\n",
    "\n",
    "# 6. Treinar o pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Avaliar o pipeline\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Acurácia no treino: {train_score:.2f}\")\n",
    "print(f\"Acurácia no teste: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
